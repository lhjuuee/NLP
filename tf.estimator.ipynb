{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensorflow\n",
    "### 1.3 Estimator\n",
    "\n",
    "When using tf.estimator, we need to build two kinds of funcionts. Model and data to be fed.\n",
    "\n",
    "First, form of model function is like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params, config):\n",
    "    \n",
    "    # Build model\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this functions,\n",
    "\n",
    "- ** features: Input data. Tensor or dictionary.\n",
    "\n",
    "- ** labels: Global true of each data. Tensor or dictionary\n",
    "\n",
    "- mode: Mode of estimator. Train, Evaluate and Predict.\n",
    "\n",
    "- params: Hyperparameters we want to apply. Dictionary.\n",
    "\n",
    "- config: Value for model\n",
    "\n",
    "After this, we can apply this model function to Estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir = ..., configs= ..., parmas= ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_dir is route to save chekcpoint. And we can implemnet this estimator like,\n",
    "\n",
    "- For training\n",
    "\n",
    "estimator.train(input_fn = ...)\n",
    "\n",
    "- For evaluating\n",
    "\n",
    "estimator.evaluate(input_fn = ...)\n",
    "\n",
    "- For predicting\n",
    "\n",
    "estimator.predict(input_fn = ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For input data\n",
    "\n",
    "For function of input data, tf.data is used. \n",
    "\n",
    "Data for each situation, train, evaluate or test. is different. So, we define each function respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    \n",
    "    # Build data pipeline\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DNN model for Sentiment Analysis\n",
    "##### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "samples = ['너 오늘 이뻐 보인다',\n",
    "           '나는 오늘 기분이 더러워',\n",
    "           '끝내주는데, 좋은 일이 있나봐',\n",
    "           '나 좋은 일이 생겼어',\n",
    "           '아 오늘 진짜 짜증나',\n",
    "           '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "labels = [[1], [0], [1], [1], [0], [1]] # It doesn't have meaning.\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define train_input_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "\n",
    "def train_input_fn():\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    dataset = dataset.repeat(EPOCH)\n",
    "    dataset = dataset.batch(1)\n",
    "    dataset = dataset.shuffle(len(sequences))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_index) + 1\n",
    "EMB_SIZE = 128\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # To set each statement be 'True'\n",
    "    \n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    embed_input = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features)\n",
    "    embed_input = tf.reduce_mean(embed_input, axis=1)\n",
    "    \n",
    "    hidden_layer = tf.keras.layers.Dense(128, activation=tf.nn.relu)(embed_input)\n",
    "    output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "    output = tf.nn.sigmoid(output_layer)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(output, labels)\n",
    "    \n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          train_op=train_op,\n",
    "                                          loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:/python/NLP/Chap_2/data_outcheckpoint/dnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001A7B4A29E10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:/python/NLP/Chap_2/data_outcheckpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.24833879, step = 1\n",
      "INFO:tensorflow:global_step/sec: 212.141\n",
      "INFO:tensorflow:loss = 0.02255575, step = 101 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 320.425\n",
      "INFO:tensorflow:loss = 0.0016633512, step = 201 (0.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.888\n",
      "INFO:tensorflow:loss = 0.00020650445, step = 301 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.909\n",
      "INFO:tensorflow:loss = 7.586872e-05, step = 401 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.086\n",
      "INFO:tensorflow:loss = 0.00018009337, step = 501 (0.200 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 600 into C:/python/NLP/Chap_2/data_outcheckpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.492363e-05.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1a7b4a07860>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_OUT_PATH = 'C:/python/NLP/Chap_2/data_out'\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir = DATA_OUT_PATH + 'checkpoint/dnn')\n",
    "estimator.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
