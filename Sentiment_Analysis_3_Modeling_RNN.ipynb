{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with IMDB dataset\n",
    "## 4. Modeling\n",
    "### 4.3 RNN\n",
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "DATA_IN_PATH = 'C:/python/NLP/Chap_4/data_for_modeling/'\n",
    "DATA_OUT_PATH = 'C:/python/NLP/Chap_4/submit/'\n",
    "\n",
    "INPUT_TRAIN_DATA_FILE = 'train_input.npy'\n",
    "LABEL_TRAIN_DATA_FILE = 'train_label.npy'\n",
    "DATA_CONFIGS_FILE = 'data_configs.json'\n",
    "\n",
    "train_input = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE, 'rb'))\n",
    "train_label = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE, 'rb'))\n",
    "\n",
    "prepro_configs = None\n",
    "\n",
    "with open(DATA_IN_PATH + DATA_CONFIGS_FILE, 'r') as f:\n",
    "    prepro_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(train_input, train_label, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 174)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data(tf.data) -> model(keras.layers) -> training(tf.estimator)\n",
    "### Define data function - tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "def mapping_fn(X, Y):\n",
    "    inputs, labels = {'x': X}, Y\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_dev, Y_dev))\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling - tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74066"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_configs['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = prepro_configs['vocab_size']\n",
    "\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "HIDDEN_STATE_DIM = 150\n",
    "DENSE_FEATURE_DIM = 150\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)(features['x'])\n",
    "    embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)\n",
    "    \n",
    "    rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]]\n",
    "    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell, \n",
    "                                       inputs=embedding_layer,\n",
    "                                       dtype=tf.float32) # without for loop\n",
    "    outputs = tf.keras.layers.Dropout(0.2)(outputs)\n",
    "    \n",
    "    hidden_layer = tf.keras.layers.Dense(DENSE_FEATURE_DIM, activation=tf.nn.tanh)(outputs[:, -1, :])\n",
    "    hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer)\n",
    "    \n",
    "    logits = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "    logits = tf.squeeze(logits, axis=-1)\n",
    "    \n",
    "    sigmoid_logits = tf.nn.sigmoid(logits)\n",
    "    \n",
    "    if PREDICT:\n",
    "        predictions = {'sentiment': sigmoid_logits}\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "    \n",
    "    if EVAL:\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(sigmoid_logits))\n",
    "        eval_metric_ops = {'acc': accuracy}\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    \n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Evaluating and Predicting - tf.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.mkdir(DATA_OUT_PATH)\n",
    "\n",
    "est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + 'checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0719 16:27:16.595093 10124 deprecation.py:323] From C:\\Anaconda_\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x21b01c0d048>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8656, 'loss': 0.3385089, 'global_step': 8442}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data = np.load(open(DATA_IN_PATH + 'test_input.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_input_data["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INPUT_DATA = 'test_input.npy'\n",
    "\n",
    "test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb'))\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": np.array(test_input_data)}, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3129"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_data[115,133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0719 17:35:42.082572 10124 deprecation.py:323] From C:\\Anaconda_\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0719 17:35:42.092564 10124 deprecation.py:323] From C:\\Anaconda_\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0719 17:35:42.677980 10124 deprecation.py:323] From C:\\Anaconda_\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([p['sentiment'] for p in est.predict(input_fn=predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TEST_ID_DATA = 'test_id.npy'\n",
    "test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'))\n",
    "ouput = pd.DataFrame(data = {\"id\": test_id, \"sentiment\": list(predictions)})\n",
    "ouput.to_csv(DATA_OUT_PATH + \"rnn_predict.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
