{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with IMDB dataset\n",
    "## 4. Modeling\n",
    "\n",
    "I am going to build 4 models\n",
    "- Logistic regression\n",
    "- Random forest\n",
    "- RNN\n",
    "- CNN\n",
    "\n",
    "### 4.1 Logistic regression\n",
    "\n",
    "This is variation of linear regression. First, linear regression which predict value with weights(parameters) for each input x,\n",
    "\n",
    "could be thought as drowing a line to minimize error. Logistic regression uses predicted value as inputs for logistic function\n",
    "\n",
    "to get 0 ~ 1 value. Its result can be thought as probability that input x is classified as specific class.\n",
    "\n",
    "In this part, I'll use word2vec and tf-idf to make input embedding bector\n",
    "\n",
    "### 4.1.1 TF-IDF - logistic regression\n",
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_OUT_PATH = 'C:/python/NLP/chap_4/data_for_modeling/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "train_data = pd.read_csv(DATA_OUT_PATH + TRAIN_CLEAN_DATA, header=0, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer='char', sublinear_tf=True, ngram_range=(1,3), max_features=5000) # Refer document\n",
    "\n",
    "X = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "y = np.array(sentiments)\n",
    "\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda_\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.859600\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:5f}\".format(lr.score(X_dev, Y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "test_data = pd.read_csv(DATA_OUT_PATH + TEST_CLEAN_DATA, header=0, quoting=3)\n",
    "\n",
    "testDataVecs = vectorizer.transform(test_data['review'])\n",
    "test_predicted = lr.predict(testDataVecs)\n",
    "print(test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SUBMIT_DATA_PATH = \"C:/python/NLP/chap_4/submit/\"\n",
    "\n",
    "if not os.path.exists(SUBMIT_DATA_PATH):\n",
    "    os.mkdir(SUBMIT_DATA_PATH)\n",
    "\n",
    "ids = list(test_data['id'])\n",
    "answer_lr = pd.DataFrame({'id': ids, 'sentiment': test_predicted})\n",
    "answer_lr.to_csv(SUBMIT_DATA_PATH + 'lgs_tfidf_answer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 word2vec - logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use word2vec, all reviews have to be diveded by word in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stuff',\n",
       " 'going',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'watched',\n",
       " 'wiz',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'maybe',\n",
       " 'want',\n",
       " 'get',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'guy',\n",
       " 'thought',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'eighties',\n",
       " 'maybe',\n",
       " 'make',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'guilty',\n",
       " 'innocent',\n",
       " 'moonwalker',\n",
       " 'part',\n",
       " 'biography',\n",
       " 'part',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'remember',\n",
       " 'going',\n",
       " 'see',\n",
       " 'cinema',\n",
       " 'originally',\n",
       " 'released',\n",
       " 'subtle',\n",
       " 'messages',\n",
       " 'mj',\n",
       " 'feeling',\n",
       " 'towards',\n",
       " 'press',\n",
       " 'also',\n",
       " 'obvious',\n",
       " 'message',\n",
       " 'drugs',\n",
       " 'bad',\n",
       " 'kay',\n",
       " 'visually',\n",
       " 'impressive',\n",
       " 'course',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'unless',\n",
       " 'remotely',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'anyway',\n",
       " 'going',\n",
       " 'hate',\n",
       " 'find',\n",
       " 'boring',\n",
       " 'may',\n",
       " 'call',\n",
       " 'mj',\n",
       " 'egotist',\n",
       " 'consenting',\n",
       " 'making',\n",
       " 'movie',\n",
       " 'mj',\n",
       " 'fans',\n",
       " 'would',\n",
       " 'say',\n",
       " 'made',\n",
       " 'fans',\n",
       " 'true',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'actual',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'bit',\n",
       " 'finally',\n",
       " 'starts',\n",
       " 'minutes',\n",
       " 'excluding',\n",
       " 'smooth',\n",
       " 'criminal',\n",
       " 'sequence',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'convincing',\n",
       " 'psychopathic',\n",
       " 'powerful',\n",
       " 'drug',\n",
       " 'lord',\n",
       " 'wants',\n",
       " 'mj',\n",
       " 'dead',\n",
       " 'bad',\n",
       " 'beyond',\n",
       " 'mj',\n",
       " 'overheard',\n",
       " 'plans',\n",
       " 'nah',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'character',\n",
       " 'ranted',\n",
       " 'wanted',\n",
       " 'people',\n",
       " 'know',\n",
       " 'supplying',\n",
       " 'drugs',\n",
       " 'etc',\n",
       " 'dunno',\n",
       " 'maybe',\n",
       " 'hates',\n",
       " 'mj',\n",
       " 'music',\n",
       " 'lots',\n",
       " 'cool',\n",
       " 'things',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'turning',\n",
       " 'car',\n",
       " 'robot',\n",
       " 'whole',\n",
       " 'speed',\n",
       " 'demon',\n",
       " 'sequence',\n",
       " 'also',\n",
       " 'director',\n",
       " 'must',\n",
       " 'patience',\n",
       " 'saint',\n",
       " 'came',\n",
       " 'filming',\n",
       " 'kiddy',\n",
       " 'bad',\n",
       " 'sequence',\n",
       " 'usually',\n",
       " 'directors',\n",
       " 'hate',\n",
       " 'working',\n",
       " 'one',\n",
       " 'kid',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'dance',\n",
       " 'scene',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'movie',\n",
       " 'people',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'one',\n",
       " 'level',\n",
       " 'another',\n",
       " 'think',\n",
       " 'people',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'try',\n",
       " 'give',\n",
       " 'wholesome',\n",
       " 'message',\n",
       " 'ironically',\n",
       " 'mj',\n",
       " 'bestest',\n",
       " 'buddy',\n",
       " 'movie',\n",
       " 'girl',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'truly',\n",
       " 'one',\n",
       " 'talented',\n",
       " 'people',\n",
       " 'ever',\n",
       " 'grace',\n",
       " 'planet',\n",
       " 'guilty',\n",
       " 'well',\n",
       " 'attention',\n",
       " 'gave',\n",
       " 'subject',\n",
       " 'hmmm',\n",
       " 'well',\n",
       " 'know',\n",
       " 'people',\n",
       " 'different',\n",
       " 'behind',\n",
       " 'closed',\n",
       " 'doors',\n",
       " 'know',\n",
       " 'fact',\n",
       " 'either',\n",
       " 'extremely',\n",
       " 'nice',\n",
       " 'stupid',\n",
       " 'guy',\n",
       " 'one',\n",
       " 'sickest',\n",
       " 'liars',\n",
       " 'hope',\n",
       " 'latter']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec vectorizing\n",
    "##### Hyperparameter for word2vec\n",
    "I am going to use gensim package to vectorize with word2vec, and hyperparameters ,below, are used for gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can track process of training word2vec by 'logging' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 07:57:37,184 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "C:\\Anaconda_\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "2019-06-15 07:57:37,210 : INFO : collecting all words and their counts\n",
      "2019-06-15 07:57:37,214 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 07:57:37,713 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2019-06-15 07:57:38,193 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2019-06-15 07:57:38,431 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2019-06-15 07:57:38,433 : INFO : Loading a fresh vocabulary\n",
      "2019-06-15 07:57:38,530 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2019-06-15 07:57:38,531 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2019-06-15 07:57:38,581 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2019-06-15 07:57:38,586 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2019-06-15 07:57:38,587 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2019-06-15 07:57:38,679 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2019-06-15 07:57:38,681 : INFO : resetting layer weights\n",
      "2019-06-15 07:57:38,912 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-06-15 07:58:02,336 : INFO : EPOCH 1 - PROGRESS: at 0.37% examples, 353 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 07:58:03,385 : INFO : EPOCH 1 - PROGRESS: at 1.02% examples, 1017 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 07:58:22,572 : INFO : EPOCH 1 - PROGRESS: at 1.67% examples, 946 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 07:58:23,989 : INFO : EPOCH 1 - PROGRESS: at 1.97% examples, 1102 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 07:58:25,273 : INFO : EPOCH 1 - PROGRESS: at 2.62% examples, 1423 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 07:58:42,932 : INFO : EPOCH 1 - PROGRESS: at 2.98% examples, 1160 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 07:58:44,882 : INFO : EPOCH 1 - PROGRESS: at 3.34% examples, 1250 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 07:58:46,524 : INFO : EPOCH 1 - PROGRESS: at 3.68% examples, 1342 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 07:59:05,132 : INFO : EPOCH 1 - PROGRESS: at 4.36% examples, 1243 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 07:59:06,820 : INFO : EPOCH 1 - PROGRESS: at 4.66% examples, 1313 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 07:59:08,491 : INFO : EPOCH 1 - PROGRESS: at 4.97% examples, 1381 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 07:59:27,209 : INFO : EPOCH 1 - PROGRESS: at 5.64% examples, 1296 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 07:59:28,935 : INFO : EPOCH 1 - PROGRESS: at 5.95% examples, 1350 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 07:59:31,533 : INFO : EPOCH 1 - PROGRESS: at 6.26% examples, 1393 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 07:59:32,986 : INFO : EPOCH 1 - PROGRESS: at 6.57% examples, 1448 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 07:59:48,658 : INFO : EPOCH 1 - PROGRESS: at 6.88% examples, 1336 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 07:59:51,474 : INFO : EPOCH 1 - PROGRESS: at 7.21% examples, 1371 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 07:59:54,165 : INFO : EPOCH 1 - PROGRESS: at 7.56% examples, 1403 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 07:59:56,693 : INFO : EPOCH 1 - PROGRESS: at 7.88% examples, 1438 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:00:10,352 : INFO : EPOCH 1 - PROGRESS: at 8.24% examples, 1363 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:00:13,090 : INFO : EPOCH 1 - PROGRESS: at 8.58% examples, 1393 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:00:15,805 : INFO : EPOCH 1 - PROGRESS: at 8.88% examples, 1422 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:00:18,557 : INFO : EPOCH 1 - PROGRESS: at 9.21% examples, 1449 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:00:31,501 : INFO : EPOCH 1 - PROGRESS: at 9.51% examples, 1388 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:00:34,831 : INFO : EPOCH 1 - PROGRESS: at 9.85% examples, 1409 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:00:38,124 : INFO : EPOCH 1 - PROGRESS: at 10.16% examples, 1429 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:00:41,601 : INFO : EPOCH 1 - PROGRESS: at 10.53% examples, 1447 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:00:52,904 : INFO : EPOCH 1 - PROGRESS: at 10.90% examples, 1406 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:00:56,508 : INFO : EPOCH 1 - PROGRESS: at 11.23% examples, 1422 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:01:01,163 : INFO : EPOCH 1 - PROGRESS: at 11.52% examples, 1430 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:01:05,535 : INFO : EPOCH 1 - PROGRESS: at 11.87% examples, 1440 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:01:15,120 : INFO : EPOCH 1 - PROGRESS: at 12.22% examples, 1414 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:01:18,990 : INFO : EPOCH 1 - PROGRESS: at 12.52% examples, 1428 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:01:23,747 : INFO : EPOCH 1 - PROGRESS: at 12.84% examples, 1434 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:01:28,340 : INFO : EPOCH 1 - PROGRESS: at 13.18% examples, 1441 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:01:36,983 : INFO : EPOCH 1 - PROGRESS: at 13.48% examples, 1424 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:01:40,545 : INFO : EPOCH 1 - PROGRESS: at 13.78% examples, 1436 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:01:45,314 : INFO : EPOCH 1 - PROGRESS: at 14.04% examples, 1441 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:01:50,776 : INFO : EPOCH 1 - PROGRESS: at 14.36% examples, 1442 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:01:58,438 : INFO : EPOCH 1 - PROGRESS: at 14.70% examples, 1431 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:02:03,213 : INFO : EPOCH 1 - PROGRESS: at 15.00% examples, 1437 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:02:08,497 : INFO : EPOCH 1 - PROGRESS: at 15.33% examples, 1439 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:02:14,139 : INFO : EPOCH 1 - PROGRESS: at 15.66% examples, 1439 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:02:20,245 : INFO : EPOCH 1 - PROGRESS: at 15.95% examples, 1437 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:02:25,150 : INFO : EPOCH 1 - PROGRESS: at 16.30% examples, 1442 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:02:30,509 : INFO : EPOCH 1 - PROGRESS: at 16.65% examples, 1443 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:02:37,020 : INFO : EPOCH 1 - PROGRESS: at 16.94% examples, 1440 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:02:41,601 : INFO : EPOCH 1 - PROGRESS: at 17.26% examples, 1445 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:02:46,863 : INFO : EPOCH 1 - PROGRESS: at 17.58% examples, 1448 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:02:53,150 : INFO : EPOCH 1 - PROGRESS: at 17.88% examples, 1445 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:03:00,273 : INFO : EPOCH 1 - PROGRESS: at 18.25% examples, 1439 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:03:03,361 : INFO : EPOCH 1 - PROGRESS: at 18.59% examples, 1450 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:03:10,039 : INFO : EPOCH 1 - PROGRESS: at 18.94% examples, 1446 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:03:16,081 : INFO : EPOCH 1 - PROGRESS: at 19.26% examples, 1445 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:03:23,953 : INFO : EPOCH 1 - PROGRESS: at 19.54% examples, 1436 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:03:26,169 : INFO : EPOCH 1 - PROGRESS: at 19.89% examples, 1451 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:03:32,118 : INFO : EPOCH 1 - PROGRESS: at 20.20% examples, 1449 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:03:39,260 : INFO : EPOCH 1 - PROGRESS: at 20.52% examples, 1444 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:03:47,122 : INFO : EPOCH 1 - PROGRESS: at 20.85% examples, 1435 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:03:54,000 : INFO : EPOCH 1 - PROGRESS: at 21.50% examples, 1453 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:04:02,005 : INFO : EPOCH 1 - PROGRESS: at 21.79% examples, 1445 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:04:09,160 : INFO : EPOCH 1 - PROGRESS: at 22.14% examples, 1439 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:04:16,104 : INFO : EPOCH 1 - PROGRESS: at 22.86% examples, 1455 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 08:04:25,449 : INFO : EPOCH 1 - PROGRESS: at 23.18% examples, 1443 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:04:31,865 : INFO : EPOCH 1 - PROGRESS: at 23.56% examples, 1440 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:04:32,960 : INFO : EPOCH 1 - PROGRESS: at 23.96% examples, 1456 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:04:37,410 : INFO : EPOCH 1 - PROGRESS: at 24.28% examples, 1460 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:04:47,521 : INFO : EPOCH 1 - PROGRESS: at 24.64% examples, 1445 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:04:53,255 : INFO : EPOCH 1 - PROGRESS: at 24.98% examples, 1445 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:04:55,132 : INFO : EPOCH 1 - PROGRESS: at 25.30% examples, 1458 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:04:59,441 : INFO : EPOCH 1 - PROGRESS: at 25.60% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:05:10,098 : INFO : EPOCH 1 - PROGRESS: at 25.94% examples, 1447 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:05:14,702 : INFO : EPOCH 1 - PROGRESS: at 26.25% examples, 1450 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:05:17,979 : INFO : EPOCH 1 - PROGRESS: at 26.57% examples, 1458 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:05:22,319 : INFO : EPOCH 1 - PROGRESS: at 26.91% examples, 1462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:05:33,527 : INFO : EPOCH 1 - PROGRESS: at 27.24% examples, 1445 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:05:36,495 : INFO : EPOCH 1 - PROGRESS: at 27.52% examples, 1453 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:05:41,886 : INFO : EPOCH 1 - PROGRESS: at 27.84% examples, 1454 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:05:44,906 : INFO : EPOCH 1 - PROGRESS: at 28.16% examples, 1462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:05:56,841 : INFO : EPOCH 1 - PROGRESS: at 28.49% examples, 1444 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:05:58,407 : INFO : EPOCH 1 - PROGRESS: at 28.84% examples, 1456 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:06:04,946 : INFO : EPOCH 1 - PROGRESS: at 29.18% examples, 1454 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:06:06,159 : INFO : EPOCH 1 - PROGRESS: at 29.53% examples, 1467 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:06:18,811 : INFO : EPOCH 1 - PROGRESS: at 29.82% examples, 1447 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:06:27,864 : INFO : EPOCH 1 - PROGRESS: at 30.45% examples, 1453 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:06:40,966 : INFO : EPOCH 1 - PROGRESS: at 31.16% examples, 1449 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:06:47,930 : INFO : EPOCH 1 - PROGRESS: at 31.84% examples, 1460 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:06:50,272 : INFO : EPOCH 1 - PROGRESS: at 32.16% examples, 1469 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:07:01,723 : INFO : EPOCH 1 - PROGRESS: at 32.49% examples, 1454 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:07:08,971 : INFO : EPOCH 1 - PROGRESS: at 33.15% examples, 1465 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:07:11,812 : INFO : EPOCH 1 - PROGRESS: at 33.49% examples, 1472 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:07:21,802 : INFO : EPOCH 1 - PROGRESS: at 33.82% examples, 1460 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:07:24,370 : INFO : EPOCH 1 - PROGRESS: at 34.14% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:07:30,363 : INFO : EPOCH 1 - PROGRESS: at 34.47% examples, 1467 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:07:34,593 : INFO : EPOCH 1 - PROGRESS: at 34.78% examples, 1471 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:07:43,663 : INFO : EPOCH 1 - PROGRESS: at 35.08% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:07:47,175 : INFO : EPOCH 1 - PROGRESS: at 35.40% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:07:52,554 : INFO : EPOCH 1 - PROGRESS: at 35.72% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:07:58,346 : INFO : EPOCH 1 - PROGRESS: at 36.03% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:08:06,183 : INFO : EPOCH 1 - PROGRESS: at 36.36% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:08:10,039 : INFO : EPOCH 1 - PROGRESS: at 36.66% examples, 1467 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:08:14,648 : INFO : EPOCH 1 - PROGRESS: at 37.01% examples, 1469 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:08:21,275 : INFO : EPOCH 1 - PROGRESS: at 37.39% examples, 1467 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:08:28,906 : INFO : EPOCH 1 - PROGRESS: at 37.76% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:08:34,439 : INFO : EPOCH 1 - PROGRESS: at 38.07% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:08:38,821 : INFO : EPOCH 1 - PROGRESS: at 38.42% examples, 1466 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:08:45,625 : INFO : EPOCH 1 - PROGRESS: at 38.76% examples, 1464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:08:51,714 : INFO : EPOCH 1 - PROGRESS: at 39.12% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:08:57,807 : INFO : EPOCH 1 - PROGRESS: at 39.47% examples, 1462 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:09:01,010 : INFO : EPOCH 1 - PROGRESS: at 39.85% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:09:08,505 : INFO : EPOCH 1 - PROGRESS: at 40.16% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:09:12,987 : INFO : EPOCH 1 - PROGRESS: at 40.48% examples, 1466 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:09:19,797 : INFO : EPOCH 1 - PROGRESS: at 40.81% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:09:22,097 : INFO : EPOCH 1 - PROGRESS: at 41.11% examples, 1470 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:09:30,748 : INFO : EPOCH 1 - PROGRESS: at 41.44% examples, 1464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:09:33,775 : INFO : EPOCH 1 - PROGRESS: at 41.79% examples, 1469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:09:42,144 : INFO : EPOCH 1 - PROGRESS: at 42.16% examples, 1464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:09:44,156 : INFO : EPOCH 1 - PROGRESS: at 42.46% examples, 1471 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:09:54,547 : INFO : EPOCH 1 - PROGRESS: at 42.76% examples, 1462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:09:56,156 : INFO : EPOCH 1 - PROGRESS: at 43.10% examples, 1470 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:10:04,871 : INFO : EPOCH 1 - PROGRESS: at 43.41% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:10:06,370 : INFO : EPOCH 1 - PROGRESS: at 43.72% examples, 1472 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:10:16,680 : INFO : EPOCH 1 - PROGRESS: at 44.05% examples, 1462 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:10:26,062 : INFO : EPOCH 1 - PROGRESS: at 44.81% examples, 1466 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:10:37,236 : INFO : EPOCH 1 - PROGRESS: at 45.44% examples, 1466 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:10:38,387 : INFO : EPOCH 1 - PROGRESS: at 45.77% examples, 1474 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:10:48,401 : INFO : EPOCH 1 - PROGRESS: at 46.13% examples, 1466 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:10:59,287 : INFO : EPOCH 1 - PROGRESS: at 46.80% examples, 1467 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:11:01,954 : INFO : EPOCH 1 - PROGRESS: at 47.12% examples, 1473 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:11:10,875 : INFO : EPOCH 1 - PROGRESS: at 47.48% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:11:21,445 : INFO : EPOCH 1 - PROGRESS: at 48.08% examples, 1468 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:11:26,221 : INFO : EPOCH 1 - PROGRESS: at 48.39% examples, 1470 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:11:34,034 : INFO : EPOCH 1 - PROGRESS: at 48.69% examples, 1466 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:11:44,042 : INFO : EPOCH 1 - PROGRESS: at 49.36% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:11:49,572 : INFO : EPOCH 1 - PROGRESS: at 49.68% examples, 1469 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:11:57,540 : INFO : EPOCH 1 - PROGRESS: at 50.00% examples, 1465 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:11:58,912 : INFO : EPOCH 1 - PROGRESS: at 50.35% examples, 1472 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 08:12:06,634 : INFO : EPOCH 1 - PROGRESS: at 50.69% examples, 1469 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:12:13,032 : INFO : EPOCH 1 - PROGRESS: at 51.03% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:12:18,949 : INFO : EPOCH 1 - PROGRESS: at 51.37% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:12:21,767 : INFO : EPOCH 1 - PROGRESS: at 51.68% examples, 1471 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:12:28,532 : INFO : EPOCH 1 - PROGRESS: at 52.03% examples, 1470 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:12:36,043 : INFO : EPOCH 1 - PROGRESS: at 52.35% examples, 1466 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:12:40,951 : INFO : EPOCH 1 - PROGRESS: at 52.60% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:12:43,915 : INFO : EPOCH 1 - PROGRESS: at 52.93% examples, 1472 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:12:50,287 : INFO : EPOCH 1 - PROGRESS: at 53.26% examples, 1471 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:12:59,856 : INFO : EPOCH 1 - PROGRESS: at 53.58% examples, 1464 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:13:03,582 : INFO : EPOCH 1 - PROGRESS: at 53.88% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:13:07,316 : INFO : EPOCH 1 - PROGRESS: at 54.20% examples, 1470 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:13:12,868 : INFO : EPOCH 1 - PROGRESS: at 54.58% examples, 1471 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:13:23,254 : INFO : EPOCH 1 - PROGRESS: at 54.91% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:13:25,322 : INFO : EPOCH 1 - PROGRESS: at 55.23% examples, 1469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:13:29,833 : INFO : EPOCH 1 - PROGRESS: at 55.57% examples, 1470 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:13:34,375 : INFO : EPOCH 1 - PROGRESS: at 55.84% examples, 1472 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:13:46,363 : INFO : EPOCH 1 - PROGRESS: at 56.18% examples, 1462 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:13:47,702 : INFO : EPOCH 1 - PROGRESS: at 56.52% examples, 1469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:13:52,738 : INFO : EPOCH 1 - PROGRESS: at 56.86% examples, 1470 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:13:56,060 : INFO : EPOCH 1 - PROGRESS: at 57.25% examples, 1473 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:14:09,833 : INFO : EPOCH 1 - PROGRESS: at 57.58% examples, 1461 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:14:16,147 : INFO : EPOCH 1 - PROGRESS: at 58.21% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:14:18,353 : INFO : EPOCH 1 - PROGRESS: at 58.55% examples, 1474 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:14:31,877 : INFO : EPOCH 1 - PROGRESS: at 58.90% examples, 1462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:14:38,593 : INFO : EPOCH 1 - PROGRESS: at 59.57% examples, 1469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:14:39,726 : INFO : EPOCH 1 - PROGRESS: at 59.90% examples, 1475 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:14:54,427 : INFO : EPOCH 1 - PROGRESS: at 60.26% examples, 1462 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:14:56,537 : INFO : EPOCH 1 - PROGRESS: at 60.64% examples, 1468 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:15:01,564 : INFO : EPOCH 1 - PROGRESS: at 60.96% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:15:16,967 : INFO : EPOCH 1 - PROGRESS: at 61.61% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:15:20,155 : INFO : EPOCH 1 - PROGRESS: at 62.00% examples, 1466 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:15:25,205 : INFO : EPOCH 1 - PROGRESS: at 62.31% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:15:39,255 : INFO : EPOCH 1 - PROGRESS: at 63.00% examples, 1464 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:15:43,241 : INFO : EPOCH 1 - PROGRESS: at 63.33% examples, 1466 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:15:47,080 : INFO : EPOCH 1 - PROGRESS: at 63.66% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:15:48,240 : INFO : EPOCH 1 - PROGRESS: at 64.00% examples, 1474 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:16:03,010 : INFO : EPOCH 1 - PROGRESS: at 64.40% examples, 1462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:16:07,384 : INFO : EPOCH 1 - PROGRESS: at 64.76% examples, 1464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:16:10,316 : INFO : EPOCH 1 - PROGRESS: at 65.10% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:16:12,691 : INFO : EPOCH 1 - PROGRESS: at 65.43% examples, 1472 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:16:26,461 : INFO : EPOCH 1 - PROGRESS: at 65.72% examples, 1461 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:16:32,154 : INFO : EPOCH 1 - PROGRESS: at 66.06% examples, 1461 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:16:33,231 : INFO : EPOCH 1 - PROGRESS: at 66.38% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:16:35,980 : INFO : EPOCH 1 - PROGRESS: at 66.71% examples, 1471 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:16:48,200 : INFO : EPOCH 1 - PROGRESS: at 67.07% examples, 1462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:16:54,556 : INFO : EPOCH 1 - PROGRESS: at 67.42% examples, 1462 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:16:58,589 : INFO : EPOCH 1 - PROGRESS: at 68.05% examples, 1471 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:17:10,119 : INFO : EPOCH 1 - PROGRESS: at 68.38% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:17:16,197 : INFO : EPOCH 1 - PROGRESS: at 68.70% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:17:17,703 : INFO : EPOCH 1 - PROGRESS: at 69.00% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:17:22,278 : INFO : EPOCH 1 - PROGRESS: at 69.33% examples, 1469 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:17:33,334 : INFO : EPOCH 1 - PROGRESS: at 69.63% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:17:38,850 : INFO : EPOCH 1 - PROGRESS: at 69.97% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:17:41,547 : INFO : EPOCH 1 - PROGRESS: at 70.29% examples, 1466 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:17:45,241 : INFO : EPOCH 1 - PROGRESS: at 70.64% examples, 1469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:17:57,726 : INFO : EPOCH 1 - PROGRESS: at 71.00% examples, 1461 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:18:02,172 : INFO : EPOCH 1 - PROGRESS: at 71.33% examples, 1462 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:18:06,826 : INFO : EPOCH 1 - PROGRESS: at 71.62% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:18:11,160 : INFO : EPOCH 1 - PROGRESS: at 71.94% examples, 1465 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:18:20,824 : INFO : EPOCH 1 - PROGRESS: at 72.32% examples, 1460 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:18:25,603 : INFO : EPOCH 1 - PROGRESS: at 72.66% examples, 1461 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:18:31,223 : INFO : EPOCH 1 - PROGRESS: at 73.01% examples, 1461 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:18:33,941 : INFO : EPOCH 1 - PROGRESS: at 73.31% examples, 1464 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:18:42,287 : INFO : EPOCH 1 - PROGRESS: at 73.60% examples, 1461 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:18:46,399 : INFO : EPOCH 1 - PROGRESS: at 73.95% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:18:52,187 : INFO : EPOCH 1 - PROGRESS: at 74.29% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:18:56,414 : INFO : EPOCH 1 - PROGRESS: at 74.61% examples, 1465 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:19:04,788 : INFO : EPOCH 1 - PROGRESS: at 74.94% examples, 1462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:19:10,626 : INFO : EPOCH 1 - PROGRESS: at 75.28% examples, 1461 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:19:17,303 : INFO : EPOCH 1 - PROGRESS: at 75.63% examples, 1460 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:19:20,487 : INFO : EPOCH 1 - PROGRESS: at 75.99% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:19:28,644 : INFO : EPOCH 1 - PROGRESS: at 76.32% examples, 1460 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:19:32,711 : INFO : EPOCH 1 - PROGRESS: at 76.69% examples, 1462 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 08:19:41,538 : INFO : EPOCH 1 - PROGRESS: at 77.05% examples, 1459 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:19:44,049 : INFO : EPOCH 1 - PROGRESS: at 77.34% examples, 1462 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:19:50,737 : INFO : EPOCH 1 - PROGRESS: at 77.68% examples, 1461 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:19:54,484 : INFO : EPOCH 1 - PROGRESS: at 78.01% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:20:03,280 : INFO : EPOCH 1 - PROGRESS: at 78.35% examples, 1460 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:20:05,101 : INFO : EPOCH 1 - PROGRESS: at 78.70% examples, 1464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:20:12,023 : INFO : EPOCH 1 - PROGRESS: at 79.04% examples, 1462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:20:16,147 : INFO : EPOCH 1 - PROGRESS: at 79.41% examples, 1464 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:20:26,195 : INFO : EPOCH 1 - PROGRESS: at 79.77% examples, 1459 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:20:27,516 : INFO : EPOCH 1 - PROGRESS: at 80.13% examples, 1464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:20:33,322 : INFO : EPOCH 1 - PROGRESS: at 80.47% examples, 1464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:20:37,915 : INFO : EPOCH 1 - PROGRESS: at 80.79% examples, 1465 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:20:49,012 : INFO : EPOCH 1 - PROGRESS: at 81.13% examples, 1459 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:20:55,005 : INFO : EPOCH 1 - PROGRESS: at 81.75% examples, 1465 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:20:58,975 : INFO : EPOCH 1 - PROGRESS: at 82.05% examples, 1467 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:21:11,333 : INFO : EPOCH 1 - PROGRESS: at 82.36% examples, 1460 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:21:15,518 : INFO : EPOCH 1 - PROGRESS: at 83.01% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:21:19,678 : INFO : EPOCH 1 - PROGRESS: at 83.34% examples, 1469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:21:32,972 : INFO : EPOCH 1 - PROGRESS: at 83.64% examples, 1461 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:21:35,894 : INFO : EPOCH 1 - PROGRESS: at 84.33% examples, 1469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:21:40,217 : INFO : EPOCH 1 - PROGRESS: at 84.64% examples, 1471 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:21:55,065 : INFO : EPOCH 1 - PROGRESS: at 84.99% examples, 1461 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:21:58,038 : INFO : EPOCH 1 - PROGRESS: at 85.63% examples, 1470 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:22:02,076 : INFO : EPOCH 1 - PROGRESS: at 85.94% examples, 1471 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:22:16,542 : INFO : EPOCH 1 - PROGRESS: at 86.28% examples, 1463 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:22:17,948 : INFO : EPOCH 1 - PROGRESS: at 86.58% examples, 1467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:22:22,942 : INFO : EPOCH 1 - PROGRESS: at 87.17% examples, 1473 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:22:38,427 : INFO : EPOCH 1 - PROGRESS: at 87.51% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:22:39,681 : INFO : EPOCH 1 - PROGRESS: at 87.86% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:22:43,621 : INFO : EPOCH 1 - PROGRESS: at 88.58% examples, 1475 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:23:01,302 : INFO : EPOCH 1 - PROGRESS: at 88.98% examples, 1463 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:23:04,128 : INFO : EPOCH 1 - PROGRESS: at 89.69% examples, 1471 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:23:05,622 : INFO : EPOCH 1 - PROGRESS: at 90.03% examples, 1475 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:23:23,191 : INFO : EPOCH 1 - PROGRESS: at 90.34% examples, 1464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:23:25,538 : INFO : EPOCH 1 - PROGRESS: at 91.01% examples, 1472 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:23:45,269 : INFO : EPOCH 1 - PROGRESS: at 91.70% examples, 1464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:23:48,036 : INFO : EPOCH 1 - PROGRESS: at 92.40% examples, 1472 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:24:07,246 : INFO : EPOCH 1 - PROGRESS: at 93.12% examples, 1465 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:24:08,793 : INFO : EPOCH 1 - PROGRESS: at 93.47% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:24:11,609 : INFO : EPOCH 1 - PROGRESS: at 94.14% examples, 1476 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:24:28,283 : INFO : EPOCH 1 - PROGRESS: at 94.46% examples, 1466 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:24:29,737 : INFO : EPOCH 1 - PROGRESS: at 94.77% examples, 1470 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:24:32,998 : INFO : EPOCH 1 - PROGRESS: at 95.46% examples, 1477 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:24:48,120 : INFO : EPOCH 1 - PROGRESS: at 95.78% examples, 1468 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:24:49,251 : INFO : EPOCH 1 - PROGRESS: at 96.07% examples, 1472 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:24:51,673 : INFO : EPOCH 1 - PROGRESS: at 96.42% examples, 1475 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:24:54,305 : INFO : EPOCH 1 - PROGRESS: at 96.76% examples, 1478 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:25:10,353 : INFO : EPOCH 1 - PROGRESS: at 97.12% examples, 1469 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:25:13,545 : INFO : EPOCH 1 - PROGRESS: at 97.78% examples, 1476 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:25:17,708 : INFO : EPOCH 1 - PROGRESS: at 98.12% examples, 1477 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-15 08:25:31,762 : INFO : EPOCH 1 - PROGRESS: at 98.46% examples, 1470 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-15 08:25:35,002 : INFO : EPOCH 1 - PROGRESS: at 99.19% examples, 1477 words/s, in_qsize 3, out_qsize 1\n",
      "2019-06-15 08:25:35,003 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-15 08:25:37,822 : INFO : EPOCH 1 - PROGRESS: at 99.30% examples, 1476 words/s, in_qsize 2, out_qsize 1\n",
      "2019-06-15 08:25:37,824 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-15 08:25:37,856 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-15 08:25:39,663 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 1484 words/s, in_qsize 0, out_qsize 1\n",
      "2019-06-15 08:25:39,664 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-15 08:25:39,664 : INFO : EPOCH - 1 : training on 2988089 raw words (2494281 effective words) took 1680.7s, 1484 effective words/s\n",
      "2019-06-15 08:26:00,020 : INFO : EPOCH 2 - PROGRESS: at 0.37% examples, 405 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:26:01,059 : INFO : EPOCH 2 - PROGRESS: at 1.01% examples, 1161 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:26:22,468 : INFO : EPOCH 2 - PROGRESS: at 1.67% examples, 963 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:26:24,366 : INFO : EPOCH 2 - PROGRESS: at 2.28% examples, 1293 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:26:43,229 : INFO : EPOCH 2 - PROGRESS: at 2.98% examples, 1167 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:26:45,967 : INFO : EPOCH 2 - PROGRESS: at 3.65% examples, 1364 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:27:04,920 : INFO : EPOCH 2 - PROGRESS: at 4.36% examples, 1257 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:27:08,368 : INFO : EPOCH 2 - PROGRESS: at 4.98% examples, 1395 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:27:27,964 : INFO : EPOCH 2 - PROGRESS: at 5.60% examples, 1295 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:27:31,833 : INFO : EPOCH 2 - PROGRESS: at 6.26% examples, 1399 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:27:49,265 : INFO : EPOCH 2 - PROGRESS: at 6.88% examples, 1339 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:27:55,957 : INFO : EPOCH 2 - PROGRESS: at 7.56% examples, 1394 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:27:58,011 : INFO : EPOCH 2 - PROGRESS: at 7.88% examples, 1433 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:28:15,403 : INFO : EPOCH 2 - PROGRESS: at 8.24% examples, 1326 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:28:16,671 : INFO : EPOCH 2 - PROGRESS: at 8.58% examples, 1369 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:28:22,672 : INFO : EPOCH 2 - PROGRESS: at 8.88% examples, 1369 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 08:28:26,011 : INFO : EPOCH 2 - PROGRESS: at 9.21% examples, 1392 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:28:41,059 : INFO : EPOCH 2 - PROGRESS: at 9.51% examples, 1322 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:28:42,233 : INFO : EPOCH 2 - PROGRESS: at 9.85% examples, 1358 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:28:46,982 : INFO : EPOCH 2 - PROGRESS: at 10.16% examples, 1368 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:28:49,862 : INFO : EPOCH 2 - PROGRESS: at 10.53% examples, 1391 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:29:03,738 : INFO : EPOCH 2 - PROGRESS: at 10.90% examples, 1337 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:29:04,858 : INFO : EPOCH 2 - PROGRESS: at 11.23% examples, 1370 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:29:11,720 : INFO : EPOCH 2 - PROGRESS: at 11.52% examples, 1365 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:29:15,346 : INFO : EPOCH 2 - PROGRESS: at 11.87% examples, 1380 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:29:28,240 : INFO : EPOCH 2 - PROGRESS: at 12.22% examples, 1338 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:29:29,527 : INFO : EPOCH 2 - PROGRESS: at 12.52% examples, 1367 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:29:35,425 : INFO : EPOCH 2 - PROGRESS: at 12.84% examples, 1369 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:29:38,635 : INFO : EPOCH 2 - PROGRESS: at 13.18% examples, 1385 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:29:49,240 : INFO : EPOCH 2 - PROGRESS: at 13.48% examples, 1359 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:29:56,575 : INFO : EPOCH 2 - PROGRESS: at 14.04% examples, 1383 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:30:00,555 : INFO : EPOCH 2 - PROGRESS: at 14.36% examples, 1393 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:30:10,100 : INFO : EPOCH 2 - PROGRESS: at 14.70% examples, 1374 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:30:11,177 : INFO : EPOCH 2 - PROGRESS: at 15.00% examples, 1399 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:30:17,945 : INFO : EPOCH 2 - PROGRESS: at 15.33% examples, 1395 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:30:22,260 : INFO : EPOCH 2 - PROGRESS: at 15.66% examples, 1403 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:30:31,449 : INFO : EPOCH 2 - PROGRESS: at 15.95% examples, 1386 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:30:33,790 : INFO : EPOCH 2 - PROGRESS: at 16.30% examples, 1404 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:30:40,628 : INFO : EPOCH 2 - PROGRESS: at 16.65% examples, 1399 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:30:45,896 : INFO : EPOCH 2 - PROGRESS: at 16.94% examples, 1402 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:30:53,250 : INFO : EPOCH 2 - PROGRESS: at 17.26% examples, 1396 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:30:55,368 : INFO : EPOCH 2 - PROGRESS: at 17.58% examples, 1413 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:31:02,991 : INFO : EPOCH 2 - PROGRESS: at 17.88% examples, 1405 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:31:09,121 : INFO : EPOCH 2 - PROGRESS: at 18.25% examples, 1404 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:31:15,141 : INFO : EPOCH 2 - PROGRESS: at 18.59% examples, 1403 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:31:17,643 : INFO : EPOCH 2 - PROGRESS: at 18.94% examples, 1417 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:31:25,816 : INFO : EPOCH 2 - PROGRESS: at 19.26% examples, 1408 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:31:32,726 : INFO : EPOCH 2 - PROGRESS: at 19.54% examples, 1404 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:31:38,434 : INFO : EPOCH 2 - PROGRESS: at 19.89% examples, 1405 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:31:40,731 : INFO : EPOCH 2 - PROGRESS: at 20.20% examples, 1419 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:31:49,979 : INFO : EPOCH 2 - PROGRESS: at 20.52% examples, 1406 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:31:56,590 : INFO : EPOCH 2 - PROGRESS: at 20.85% examples, 1403 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:32:00,283 : INFO : EPOCH 2 - PROGRESS: at 21.17% examples, 1411 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:32:02,933 : INFO : EPOCH 2 - PROGRESS: at 21.50% examples, 1423 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:32:12,585 : INFO : EPOCH 2 - PROGRESS: at 21.79% examples, 1409 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:32:19,109 : INFO : EPOCH 2 - PROGRESS: at 22.16% examples, 1407 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-15 08:32:21,475 : INFO : EPOCH 2 - PROGRESS: at 22.50% examples, 1419 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:32:23,693 : INFO : EPOCH 2 - PROGRESS: at 22.86% examples, 1431 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:32:35,454 : INFO : EPOCH 2 - PROGRESS: at 23.18% examples, 1411 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:32:43,932 : INFO : EPOCH 2 - PROGRESS: at 23.56% examples, 1403 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:32:45,049 : INFO : EPOCH 2 - PROGRESS: at 23.96% examples, 1418 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-15 08:32:47,313 : INFO : EPOCH 2 - PROGRESS: at 24.28% examples, 1430 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-65cb19656bac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                           \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_word_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                           \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                           sample=downsampling)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda_\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[1;32mC:\\Anaconda_\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda_\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda_\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda_\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32mC:\\Anaconda_\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda_\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda_\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda_\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                          workers=num_workers,\n",
    "                          size=num_features,\n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word2vec model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec -> Input\n",
    "\n",
    "Now, word2vec needs to be proper shape for input. Each reviews has different number of words, so we need to make them same shape.\n",
    "\n",
    "One of the simple method to deal with this is to get average of each review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            \n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            \n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "        \n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, evaluation and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = test_data_vecs\n",
    "Y = np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_Weight='balanced')\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Accuracy: %f\" % format(lr.score(X_dev, Y_dev)))\n",
    "\n",
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "test_data = pd.read_csv(DATA_OUT_PATH + TEST_CLEAN_DATA)\n",
    "test_reviews = list(test_data['review'])\n",
    "test_sentences = []\n",
    "for review in test_data:\n",
    "    test_sentenes.append(review.split())\n",
    "    \n",
    "test_data_vecs = get_dataset(test_sentences, model, num_features)\n",
    "\n",
    "DATA_OUT_PATH = 'C:/python/NLP/Chap_4/submit/'\n",
    "test_predicted = lr.predict(test_data_vecs)\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiments': test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
